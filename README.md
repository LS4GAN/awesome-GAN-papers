# Awesome GAN papers

By all means, this is not a complete list of GAN-related papers.  We are
specifically looking for generative modeling techniques to bridge the gap
between two domains, something similar to the CycleGAN. This repo also includes
historical important GAN papers, their difficulties in training,
and various techniques to overcome such difficulties and improve the performance.

## GANs between domains
In reverse chronological order 

* [StarGANv2](https://arxiv.org/abs/1912.01865)
* [CartonGAN](https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf)
* [CycleGAN](https://arxiv.org/abs/1703.10593)

## Interpreting GANs

* [White-box Cartoonization](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Learning_to_Cartoonize_Using_White-Box_Cartoon_Representations_CVPR_2020_paper.pdf)

## GANs for high-resolution images
In reverse chronological order 

* [StyleGANv2](https://arxiv.org/abs/1912.04958)
* [BigGAN](https://arxiv.org/pdf/1806.06778.pdf)

## Classic GANs
In chronological order. 

* [GAN](https://arxiv.org/pdf/1406.2661.pdf)
* [GAN Tutorial](https://arxiv.org/pdf/1701.00160.pdf)
* [CGAN](https://arxiv.org/abs/1411.1784)
* [DCGAN](https://arxiv.org/abs/1511.06434)
* [WGAN](https://arxiv.org/abs/1701.07875)

## The other side of the coin
It is hard to find a paper dedicated on the failures of the GAN techniques.  But
some papers may have some content describing what problems the previous GAN
has.

* [Fig1,Fig6,StyleGANv2](https://arxiv.org/abs/1912.04958)
* [Intro,WGAN](https://arxiv.org/abs/1701.07875)

## The details matter
There are many techniques to improve GAN's performance. And some of them are
tried and true. 

* [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)
